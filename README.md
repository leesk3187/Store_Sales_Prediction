# 1. 문제 정의 (Project Objective)

본 프로젝트의 핵심 목표는 매장별/일자별 메뉴 판매 데이터를 기반으로 **미래 판매량을 예측하는 AI 모델을 구축**한다.

**"어떤 요인이 판매량에 영향을 미치며, 왜 그런 결과가 나타나는가?"** 에 대한 논리적이고 해석 가능한 근거를 제시하는 데 중점을 둔다. 이를 위해 다음과 같은 세부 질문에 답하고자 한다다.

### 주요 분석 질문

1. **핵심 예측 (Prediction):** 매장의 고유 특성(상권), 시시각각 변하는 외부 환경(날씨, 인구), 그리고 메뉴 자체의 특성을 종합하여 미래의 메뉴별 판매량을 얼마나 정확하게 예측할 수 있는가?
2. **요인 분석 (Interpretation):** 판매량에 영향을 미치는 여러 요인들 중,
    - 외부 환경 요인 (e.g., 기온, 강수량, 유동인구)
    - 메뉴 고유의 특성 (e.g., '뜨거운', '차가운', '매운맛')
    - 시간적 요인 (e.g., 요일, 공휴일, 과거 판매량)
    ...중 어떤 변수 그룹이 판매량 예측에 가장 결정적인 영향을 미치는가?
3. **상호작용 분석 (Interaction):** 메뉴명에 숨겨진 의미론적 속성(Semantic Feature)과 특정 외부 환경 조건이 결합되었을 때 판매량에 어떤 시너지 효과가 발생하는가?

### 접근 방식 및 차별점

본 분석에서는 자연어 처리 모델인 **SBERT(Sentence-BERT)** 를 도입하여, `배)1인 설렁탕+갈비만두`, `HMR)미역국`, `(배달)콜라`와 같이 접두사, 세트 구성, 비-제품 항목이 혼재된 복잡한 `메뉴명` 텍스트를 의미론적 벡터(Vector)로 변환한다.

이는 `is_set`, `is_fee` 등으로 사전에 정의한 피처(Feature)를 보완하는 역할을 한다. SBERT를 통해 텍스트 자체에서 숨겨진 패턴(e.g., '설렁탕' 계열 메뉴들의 유사성, 'HMR' 제품군의 공통점)을 모델이 스스로 학습하고 정량화하도록 유도한다. 이 벡터는 메뉴 간의 복잡한 관계를 이해하는 핵심 피처로 사용된다.

### 최종 목표

분석 결과를 바탕으로 데이터에 기반한 비즈니스 의사결정을 지원하는 것을 목표로 한다.
- **수요 예측:** 날씨 예보 및 인구 데이터를 기반으로 메뉴별 수요를 예측하여 효율적인 재고 관리 방안 제시
- **프로모션 전략:** 특정 조건 하에서 판매량이 급증하는 메뉴 '그룹'을 식별하여 타겟 마케팅 및 프로모션 기회 발굴

---

# 2. 데이터 수집 및 탐색적 데이터 분석 (EDA)

`sample_data` 외에 주어진 '외부데이터 활용 가이드' 따라 다음과 같은 3가지 외부 데이터를 수집한다다.

### 2.1. 수집 데이터 목록

**1. 날씨 데이터**

- **Why?** 날씨는 메뉴 선택(e.g., 더운 날 차가운 음료)과 외출 여부에 직접적인 영향을 미치는 핵심 변수이다다.
- **주요 논리:**
    - `sample_data`가 2025년 10월 15일까지 존재하나, 수집 가능한 날씨 데이터는 9월 30일까지다. 데이터 왜곡을 방지하고 무결성을 지키기 위해, **전체 분석 기간을 `2023-01-01 ~ 2025-09-30` (총 1004일)로 확정**한다.
    - 2025년 데이터는 '분' 단위로만 제공되어, 2023-24년 '일' 데이터와 형식을 맞추기 위해 '일' 단위(평균/최저/최고기온, 일강수량)로 가공한다.
- **수집 항목:** 일 평균/최저/최고 기온, 일 강수량
- **수집처:** 기상청 기상자료개방포털 (종관기상관측 ASOS, 서울(108) 지점)
- [Link](https://data.kma.go.kr/data/grnd/selectAsosRltmList.do?pgmNo=36)

**2. 인구 데이터**

- **Why?** 상주/유동/거주 인구는 요일별, 시간대별(e.g., 평일 점심) 매출 패턴을 결정하는 주요 요인이이다.
- **주요 논리:**
    - 과제 가이드('회사 반경 1km')에 따라 '삼성동'의 행정동 데이터를 수집 기준으로 설정한다.
    - 수집 가능한 최신 데이터가 2025년 2분기까지라, 3분기 데이터는 누락된 상태. (이는 `3. 데이터 전처리` 단계에서 해결)
- **수집 항목 (분기별):** 유동인구, 상주(직장)인구, 거주인구
- **수집처:** 서울 열린데이터 광장 (우리마을가게 상권분석서비스)
- [Link](https://data.seoul.go.kr/dataList/OA-22178/S/1/datasetView.do#AXexec)

**3. 특수일 데이터**

- **Why?** 공휴일이나 명절 연휴는 일반적인 요일 패턴을 벗어나는 이벤트 변수이다.
- **주요 논리 (is_event 기준):** 법정 공휴일(`is_holiday`) 외, 소비 패턴에 영향을 주는 기간을 `is_event`로 정의한다.
    - **장기 연휴 (5일 이상):** 귀성/여행을 고려하여 연휴 기간 전후 1~2일을 이벤트로 지정.
    - **단기 명절 및 연말:** 소비 집중을 고려하여 연휴 기간 전후 1일 및 연말(12/24, 12/31)을 이벤트로 지정.
- **수집 항목:** `is_holiday` (법정 공휴일), `is_event` (자체 정의 이벤트)
- **수집처:** Python `holidays` 라이브러리 및 자체 정의


### 2.2. 탐색적 데이터 분석 (EDA)

수집한 모든 내부/외부 데이터는 `3. 데이터 전처리` 단계에서 통합 및 정제된다.

통합된 `final_integrated_dataset.csv`를 기반으로 EDA를 진행하여 변수 간의 관계, 데이터 분포, 시계열 패턴 등을 시각화하고 분석 전략을 수립한다. (EDA 상세 내용은 `05_EDA_and_Feature_Engineering.ipynb` 노트북 참조)

# 3.데이터 전처리
`04_data_integration.ipynb`에서 6개의 데이터 소스를 하나로 병합하고, 모델링을 위해 결측값들을 논리적 근거에 기반하여 처리.

### 3.1. 데이터 통합

- **기준 데이터:** `sample_data_1016.csv` (판매 데이터)를 `df_main`으로 하여 `how='left'` 조인의 기준으로 지정정.
- **병합 키 (Keys):**
    - **`date` (일자):** `날씨`, `공휴일` 데이터를 병합하는 기본 키로 사용했습니다.
    - **`year`, `month`, `quarter`:** `date` 컬럼에서 파생하여 `상권지수`, `매장지수`, `인구` 데이터를 병합하는 키로 사용했습니다.
    - **`매장명` + `상권`:** `sample_data`의 `매장명`과 `매장지수`의 `매장코드`가 'Store_0**' 형식으로 일치함을 **EDA로 확인**하였으며, 이를 `상권` 컬럼과 조합하여 매장을 식별하는 고유 키로 사용했습니다.

### 3.2. 결측값 처리 (Logical Imputation)

병합 과정에서 발생한 결측값(`NaN`)들을 **데이터 왜곡을 최소화**하는 방향으로 각 데이터의 특성에 맞춰 논리적으로 추론하여 처리한다.

| **데이터** | **결측 원인** | **처리 기준 (왜곡 최소화)** |
| --- | --- | --- |
| **`상권지수`** | **2025년 데이터만 존재** (23-24년 `NaN`) | **[최신 데이터로 과거 보정]** `groupby(['상권', 'month'])`로 묶어, 2025년의 값을 2023-24년으로 `bfill()` (거꾸로 채우기) |
| **`매장코드`** | **일부 매장 코드('Store_001' 등) 누락** | **[상권/월 평균으로 추론]** 누락된 매장은 '같은 상권의 같은 월 평균 지수'를 따를 것으로 가정, `groupby(['상권', 'month'])`의 `mean()` 값으로 `fillna()` |
| **`인구 데이터`** | **2025년 3분기 데이터 누락** | **[직전 데이터로 보정]** 인구는 급변하지 않으므로, 직전 분기(2025-Q2)의 값으로 `ffill()` (앞으로 채우기) |
| **`날씨 데이터`** | **일부 날짜(행) 누락** | **[선형 보간법]** 날씨는 연속적이므로, 누락된 날짜의 앞뒤 날씨 값을 `interpolate()` (선형 보간) |
| **`메뉴 데이터`** | **원본 `sample_data_1016.csv` 누락** | **[범주화]** 삭제 대신 `fillna('UNKNOWN')`으로 처리하여, "정보 없음"을 모델이 학습할 수 있는 하나의 범주 생성 |

### 3.3. 피처 엔지니어링
EDA 과정에서 `배달료`, `행사` 등 **제품이 아닌 항목(Non-product)**이 포함되어 데이터에 심각한 노이즈(Noise)를 유발함을 발견하여 다음과 같이 작업한다.

모델이 데이터의 복잡한 특성을 명확히 학습할 수 있도록 `05_Feature_Engineering.ipynb`에서 다음과 같이 **신규 피처 5개**를 설계하고 생성한다다.

1. **`is_fee` (수수료/비용 여부):**
    - '배달료', '배달팁', '행사' 등 실제 제품이 아닌 항목을 구분한다.
    - **키워드:** `배달료|배달팁|배달비|행사|보냉백`
2. **`is_set` (세트 메뉴 여부):**
    - '1+1' 또는 '설렁탕+만두' 같은 조합/세트 메뉴를 구분한다.
    - **키워드:** `세트|\+`
3. **`is_side` (사이드 메뉴 여부):**
    - '공기밥', '사리', '추가' 등 메인 메뉴와 함께 주문되는 저가 메뉴를 구분한다.
    - **키워드:** `추가|공기밥|사리`
4. **`is_delivery_item` (배달/포장 전용 여부):**
    - `배)`나 `(배달)`처럼 홀 메뉴와 구분되는 전용 메뉴를 식별한다.
    - **키워드:** `배\)|(배달)|포장|냉장|HMR`
5. **`unit_price` (제품 단가):**
    - `is_fee`가 0인, 즉 **"제품"인 경우에만** `매출액 / 수량`을 계산하고, 수수료 항목은 `0`으로 처리하여 노이즈를 제거한다.

위 과정을 통해 모델 학습에 사용할 최종 데이터셋 `model_ready_dataset.csv`를 생성한다.

# 4. 데이터 모델링

`06_Modeling.ipynb`에서 최종 정제된 데이터셋(`model_ready_dataset.csv`)을 사용하여 판매량(`수량`) 예측 AI 모델을 구축하고 학습한다. 주요 단계는 다음과 같다.

### 4.1. SBERT 피처 생성 시도 및 결정

- **초기 전략:** 과제 목표('논리적 접근' 강조) 및 사전 논의에 따라, `메뉴명`의 복잡한 텍스트 정보를 벡터화하기 위해 SBERT(`paraphrase-multilingual-MiniLM-L12-v2`, 384차원) 피처 생성을 **우선 시도**한다.
- **발생 문제:**
    1. **호환성 오류:** `protobuf`와 `tensorflow` (transformers 라이브러리 의존성) 간 버전 충돌 발생 → `protobuf==3.20.3`으로 다운그레이드하여 해결.
    2. **메모리 오류 (`MemoryError`):** 전체 데이터(약 240만 행)에 384차원 벡터를 적용/병합하는 과정에서 **16GB RAM 환경의 물리적 한계**로 메모리 부족 발생.
- **검증:** 메모리 문제 해결 및 SBERT 효과 검증을 위해 `float32` 타입 변환 및 **20% 데이터 샘플링(약 50만 행)** 으로 테스트 진행 → **성공적으로 SBERT 피처 생성 및 모델 학습 완료** (샘플 RMSE: 6.13, SBERT 피처 중요도 확인).
- **최종 결정:** 16GB RAM 환경에서 전체 데이터 대상 SBERT 적용이 불가능함을 확인. 제한된 자원 내 안정적인 모델 구축을 위해, SBERT 피처는 **최종 모델에서 제외**하고, 대신 `3.3. 피처 엔지니어링`에서 생성한 **수동 피처 5종(`is_fee` 등)을 핵심으로 사용하기로 결정**한한다. (이 결정 과정은 보고서에 상세히 기술)

### 4.2. 추가 피처 엔지니어링 (시간 & Lag)

모델이 시간적 패턴과 과거 의존성을 학습하도록 피처를 추가 생성한한다.

- **시간 피처:** `date` 컬럼에서 `day_of_week`(요일), `is_weekend`(주말 여부), `week_of_year`(주차) 추출.
- **Lag 피처 (핵심):** 시계열 예측 성능 향상을 위해 `groupby(['매장명', '메뉴명'])` 후 `.shift(7)`을 적용하여 **`lag_7_days_qty` (7일 전 판매량)** 피처 생성. (초기 NaN은 0으로 처리)

### 4.3. 데이터 변환 (Label Encoding)

AI 모델은 문자열(`object`)을 처리할 수 없으므로, `sklearn.preprocessing.LabelEncoder`를 사용하여 범주형 변수들을 숫자형으로 변환한한다.

- **대상 컬럼:** `상권`, `홀배달여부`, `매장명`, `메뉴코드`
- **결과:** 각 고유 문자열 값에 0부터 시작하는 정수가 매핑됨 (e.g., `홀배달여부`: '배달'→0, '포장'→1, '홀'→2).

### 4.4. 훈련 / 검증 데이터 분리

시계열 데이터의 특성을 유지하기 위해 시간순서대로 데이터를 분리한다.

- **기준:** `2025-08-01`
- **훈련(Train) 데이터:** `2023-01-01` ~ `2025-07-31` (약 224만 행)
- **검증(Validation) 데이터:** `2025-08-01` ~ `2025-09-30` (약 15만 행)
- 최종적으로 `X_train`, `y_train`, `X_val`, `y_val` 생성 완료.

### 4.5. 모델 학습 (LightGBM)

준비된 피처들을 사용하여 **LightGBM Regressor** 모델을 학습시켰습니다.

- **주요 파라미터:** `objective='regression_l1'`, `metric='rmse'`, `n_estimators=2000`, `learning_rate=0.05`
- **과적합 방지:** `eval_set=(X_val, y_val)`과 `early_stopping_rounds=100` 옵션을 사용하여, 검증 데이터 성능이 100번 연속 개선되지 않으면 학습을 조기 종료하도록 설정한한다.
- **결과:** 성공적으로 학습이 완료된 `lgb_model` 객체 확보. (최종 RMSE 및 피처 중요도는 `5. 모델 평가 및 해석` 섹션 참조)

---
## 5. 모델 평가 및 해석 (Model Evaluation & Interpretation)

`06_Modeling.ipynb`에서 학습된 LightGBM 모델의 성능을 검증 데이터셋(`2025-08-01` ~ `2025-09-30`) 기준으로 평가하고, 모델의 예측 경향성을 해석한다.

### 5.1. 예측 성능 지표

- **RMSE (Root Mean Squared Error): 6.7157**
    - 모델의 예측값이 실제 판매량(`수량`)과 **평균적으로 약 ±6.72개**의 오차를 보인다.
- **MAE (Mean Absolute Error): 2.6502**
    - 개별 예측 건들의 절대 오차 평균은 약 **±2.65개**로, RMSE보다 낮다. 모델이 대부분의 경우 비교적 정확하게 예측하지만 가끔씩 큰 오차를 보이는 경우가 있음을 시사한다.
- **R² (Coefficient of Determination): 0.5862**
    - 모델이 예측 대상인 판매량 변동성의 약 58.62%를 학습된 피처들로 설명 가능함을 의미한다. 복잡한 실제 판매량 예측 문제에서 준수한 설명력을 보인다.

### 5.2. 피처 중요도 (Feature Importance)

모델이 예측을 위해 어떤 정보를 중요하게 사용했는지 분석한다.

- **주요 변수:** `메뉴코드`, `매장명`, `unit_price`, `홀배달여부`, `lag_7_days_qty`가 예측에 가장 큰 영향을 미친다. 이는 **어떤 메뉴**를 **어떤 매장**에서 **어떤 가격**으로 **어떻게 판매**했는지, 그리고 **지난주 판매량**이 미래 예측에 핵심임을 시사합니다.
- **기타 변수:** `day_of_week`(요일), `is_delivery_item`(배달전용), 날씨(`temp_max`), 매장 특성(`매장지수`) 등 다양한 피처들이 복합적으로 예측에 기여했다.

### 5.3. 오차 분석 (Error Analysis)

모델의 예측 오차(`실제값 - 예측값`) 패턴을 분석하여 모델의 강점과 약점을 파악한다.

- **전반적 경향 (과소 예측):**
    - `실제값 vs 예측값` 시계열 그래프 및 `날짜별 평균 오차` 그래프 분석 결과 모델이 검증 기간 동안 **전반적으로 실제 판매량보다 낮게 예측(과소 예측)**하는 경향을 보인다.
    - `오차 분포 히스토그램`은 0 근처에 예측이 집중되어 대부분 정확하지만, + 방향으로 긴 꼬리(Tail)가 존재하여 가끔 큰 과소 예측 오차가 발생함을 보여준다.
- **주요 오차 발생 지점:**
    - **과소 예측:** 특정 매장(`매장명 31` 등)의 **'설렁탕'** 메뉴에서 실제 판매량이 예측치를 크게 상회하는 경우가 다수 발견되었다. (최대 오차 +357개) 모델이 포착하지 못한 특별 할인이나 이벤트의 영향일 수 있다.
    - **과대 예측:** **'설렁탕'의 다양한 변형 메뉴**(`[40년 전통]`, `(포장행사)`, `(밥따로)` 등)에서 예측치가 실제 판매량보다 높은 경우가 많았다. 모델이 메뉴명의 미묘한 차이가 판매량에 미치는 영향을 충분히 학습하지 못했을 가능성이 있다.
    - **요일별:** 모든 요일에서 과소 예측 경향이 있으며, 특히 **주말(토/일)**에 평균 오차가 더 크게 나타나, 주말 판매량 급증 패턴을 모델이 완전히 따라가지 못함을 시사한다.

종합적으로 모델은 판매량의 기본적인 패턴은 잘 학습했으나, 특정 메뉴(특히 설렁탕 계열)나 특정 상황(주말 피크, 이벤트 등)에서의 예측 정확도를 개선할 여지가 있다.

# 6.시각화 및 결과 공유



# 디렉토리 구조
📁 내_분석_프로젝트/
 │
 ├── 📁 data/
 │   ├── 📁 raw/
 │   │   └── sample_data_1016.csv       # 원본 데이터
 │   │   └── 매장지수.xlsx
 │   │   └── 상권권지수.xlsx
 │   └── 📁 processed/
 │       └──       # 전처리 후 저장한 데이터
 │
 ├── 📁 notebooks/
 │   └── main.ipynb # 분석/실험용 노트북 파일
 │
 ├── 📁 src/ (또는 scripts/)
 │   └── data_utils.py                 # 완성된 함수, 클래스 코드 (.py)
 │
 ├── 📁 reports/ (또는 output/)
 │   └── images/
 │   │   └── .png   # 분석 결과로 만든 시각화 자료
 │   └── .html     # 최종 보고서
 │
 └── README.md                          # 프로젝트 설명서